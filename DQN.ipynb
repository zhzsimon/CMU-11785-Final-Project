{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gWqlJc6OUsY",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as weight_init\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import copy\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque\n",
    "# import ipdb\n",
    "\n",
    "from tqdm import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGBu_cm0DxTd"
   },
   "source": [
    "# LSTM Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJqaSKEWDzc3"
   },
   "outputs": [],
   "source": [
    "class LibriSamples(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data, lookback_period, prediction_period):\n",
    "        \n",
    "        self.training_period = data.shape[0]\n",
    "        self.lookback_period = lookback_period\n",
    "        self.prediction_period = prediction_period\n",
    "\n",
    "        historical = []\n",
    "        label_close = []\n",
    "        \n",
    "        # use data look back n days to predict data in theiode next m days\n",
    "        # historical: (245, 5, 4) (data_size/batch_size, seq_len, input_size/# features)\n",
    "        # label_close: (245, 5, 1) (data_size/batch_size, seq_len, input_size/# features)\n",
    "        for day in range(self.training_period - lookback_period - prediction_period + 1):\n",
    "            historical.append(data[day:day + lookback_period, :])\n",
    "            label_close.append(data[day + lookback_period:day + lookback_period + prediction_period, 3:4])\n",
    "        \n",
    "        self.historical = np.array(historical, dtype=\"float32\")\n",
    "        self.label_close = np.array(label_close, dtype=\"float32\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.training_period - self.lookback_period - self.prediction_period + 1\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        return torch.from_numpy(self.historical[ind, :, :]), torch.from_numpy(self.label_close[ind, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nkQac-0DspOP"
   },
   "outputs": [],
   "source": [
    "class TestLibriSamples(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data, lookback_period):\n",
    "        \n",
    "        self.training_period = data.shape[0]\n",
    "        self.lookback_period = lookback_period\n",
    "\n",
    "        historical = []\n",
    "\n",
    "        for day in range(self.training_period - lookback_period + 1):\n",
    "            historical.append(data[day:day + lookback_period, :])\n",
    "\n",
    "        self.historical = np.array(historical, dtype=\"float32\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.training_period - self.lookback_period + 1\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        return torch.from_numpy(self.historical[ind, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3CnCXwFD2Ib",
    "outputId": "cde6ba45-fbf7-4cb4-c2ff-e182ae9c8b94"
   },
   "outputs": [],
   "source": [
    "class LSTMNetwork(nn.Module):\n",
    "    def __init__(self, n_features):  # input shape: B, T, C\n",
    "        super(LSTMNetwork, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=256, batch_first=True, bias=True, num_layers=3, dropout=0.1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.classification1 = nn.Linear(256, 128)\n",
    "        self.classification2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1, (out2, out3) = self.lstm(x)\n",
    "\n",
    "        out = self.classification1(out1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.classification2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# load data\n",
    "# change csv as necessary, e.g., 'AMZN.csv' is for analyzing Amazon's stock\n",
    "raw_data = pd.read_csv('AAPL.csv')\n",
    "n_features = len(raw_data.columns) - 4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = LSTMNetwork(n_features).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Train Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vd5bg5WyEDZB",
    "outputId": "12257abf-a0a4-430f-a423-4deeabab589d"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size = 64\n",
    "lookback_period = 5\n",
    "prediction_period = 5\n",
    "\n",
    "# min-max normalization for data pre-processing\n",
    "combined_raw_data = raw_data\n",
    "closed_max = combined_raw_data['Adj Close'].max()\n",
    "closed_min = combined_raw_data['Adj Close'].min()\n",
    "combined_scaled_data = combined_raw_data.iloc[:, 1:].apply(lambda x : (x - min(x)) / (max(x) - min(x)))\n",
    "\n",
    "# range choices for train-test split\n",
    "start = int(len(combined_scaled_data) * 0.4)\n",
    "end = int(len(combined_scaled_data) * 0.1)\n",
    "\n",
    "# Train Data Loading\n",
    "train_data = LibriSamples(combined_scaled_data.values[:-end, :-3], lookback_period, prediction_period)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "train_loader_no_shuffle = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Validation Data Loading\n",
    "val_data = TestLibriSamples(combined_scaled_data.values[-end:, :-3], lookback_period)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LsT_ZWdrF8P9"
   },
   "outputs": [],
   "source": [
    "# test train loader\n",
    "print(len(train_loader))\n",
    "for i, data in enumerate(train_loader):\n",
    "    x, y = data\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkAF9qInDFLQ"
   },
   "outputs": [],
   "source": [
    "# test validation loader\n",
    "print(len(val_loader))\n",
    "for i, data in enumerate(val_loader):\n",
    "    x = data\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6UwyTmuHOBI",
    "outputId": "cd156703-995e-4475-fd01-68de10690aba"
   },
   "outputs": [],
   "source": [
    "# training settings for LSTM-training\n",
    "epochs = 20\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00015)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BhK96O9rHU5k",
    "outputId": "e1af2684-4560-47e4-f06a-4ec613c4e810"
   },
   "outputs": [],
   "source": [
    "# training the LSTM\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = data\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(x)  # output-shape: (B, T, C)\n",
    "            loss = criterion(output, y)\n",
    "        total_loss += float(loss)\n",
    "\n",
    "        scaler.scale(loss).backward() \n",
    "        scaler.step(optimizer)\n",
    "        scheduler.step()\n",
    "        scaler.update()\n",
    "    print(\"Epoch {}/{}, Train Loss {:.04f}\".format(epoch + 1, epochs, float(total_loss / len(train_loader))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NB49Q8YrXDCO"
   },
   "outputs": [],
   "source": [
    "# generate train predictions in a non-shuffle manner\n",
    "model.eval()\n",
    "train_result = []\n",
    "for i, data in enumerate(train_loader_no_shuffle):\n",
    "    x, _ = data\n",
    "    x = x.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(x)  # output-shape: (B, T, C)\n",
    "\n",
    "    output = output.detach().cpu().numpy()\n",
    "    output = [out.mean() for out in output]\n",
    "    train_result.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yYqzv30BG1Uk",
    "outputId": "d8c3f67a-33a0-456b-ae29-3803efc3b4ff"
   },
   "outputs": [],
   "source": [
    "# revert min-max normalization for train predictions\n",
    "flat_result_train = [item for sublist in train_result for item in sublist]\n",
    "print(flat_result_train[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "id": "1IVBKff6VxSK",
    "outputId": "a6ab3bc1-4e42-4572-b7b8-63f4b51def81"
   },
   "outputs": [],
   "source": [
    "# plot the actual and predicted training data\n",
    "import matplotlib.pyplot as plt\n",
    "inversed_result_train = [(price * (closed_max - closed_min) + closed_min) for price in flat_result_train]\n",
    "actual_price = combined_raw_data['Adj Close'].values[:-end].tolist()\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "plt.plot(inversed_result_train[:], 'r', label='pred')\n",
    "plt.plot(actual_price, 'b', label='actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAzCOzbmtuP2"
   },
   "outputs": [],
   "source": [
    "# generate validation predictions\n",
    "model.eval()\n",
    "result = []\n",
    "for i, data in enumerate(val_loader, 0):\n",
    "    x = data\n",
    "    x = x.to(device)\n",
    "    # print(x)\n",
    "    with torch.no_grad():\n",
    "        output = model(x)\n",
    "        output = output.cpu().numpy()\n",
    "        output = [out.mean() for out in output]\n",
    "        result.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1UzbZqCT6sC",
    "outputId": "7b414ca6-6d99-4477-ea55-abb8e9930857"
   },
   "outputs": [],
   "source": [
    "# revert min-max normalization for validation predictions\n",
    "flat_result_val = [item for sublist in result for item in sublist]\n",
    "print(flat_result_val[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "id": "ZvoyqPGtt1aM",
    "outputId": "46f74e5f-755c-4174-dba7-b55d5197a667"
   },
   "outputs": [],
   "source": [
    "# plot the actual and predicted validation data\n",
    "inversed_result_val = [(price * (closed_max - closed_min) + closed_min) for price in flat_result_val]\n",
    "actual_price = combined_raw_data['Adj Close'].values[-end:].tolist()\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "plt.plot(inversed_result_val[:], 'r', label='pred')\n",
    "plt.plot(actual_price, 'b', label='actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oI3Dml5pov6",
    "outputId": "14c1b30f-1792-4714-f94f-db4eda178942"
   },
   "source": [
    "# Padding and Appending LSTM Results to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49IaYDtPyztJ"
   },
   "outputs": [],
   "source": [
    "# padding LSTM results with 0 \n",
    "# Since LSTM uses past n days of data to predict the future, the LSTM results\n",
    "# hence cannot match with the first and last couple days of the data\n",
    "padded_inversed_result_train = np.pad(inversed_result_train, (4, 5), constant_values=0)\n",
    "padded_flat_result_train = np.pad(flat_result_train, (4, 5), constant_values=0)\n",
    "\n",
    "padded_inversed_result_val = np.pad(inversed_result_val, (4, 0), constant_values=0)\n",
    "padded_flat_result_val = np.pad(flat_result_val, (4, 0), constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2deSMt7nCld"
   },
   "outputs": [],
   "source": [
    "combined_raw_data['LSTM Result'] = np.concatenate((padded_inversed_result_train, padded_inversed_result_val))\n",
    "combined_scaled_data['LSTM Result'] = np.concatenate((padded_flat_result_train, padded_flat_result_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace padded 0 values with Adj Close\n",
    "combined_raw_data['LSTM Result'] = np.where(combined_raw_data['LSTM Result'] == 0.0, \n",
    "                                            combined_raw_data['Adj Close'], \n",
    "                                            combined_raw_data['LSTM Result']).astype('float64')\n",
    "combined_scaled_data['LSTM Result'] = np.where(combined_scaled_data['LSTM Result'] == 0.0, \n",
    "                                               combined_scaled_data['Adj Close'], \n",
    "                                               combined_scaled_data['LSTM Result']).astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5iSdUfeDu3S"
   },
   "source": [
    "# DRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQkOKW_VOUsa"
   },
   "source": [
    "# DQN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPlKKlniOUsc",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_size, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(256, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(16, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjBTBleMOUsc"
   },
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vm5YMRohOUsd",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# replay memory for the experience replay\n",
    "# agent's memory samples will be randomly chosen from the replay memory to train the policy network\n",
    "# experience replay allows for better gradient descent performance\n",
    "\n",
    "from collections import namedtuple\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'next_state'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9IzPSwXOUsd"
   },
   "source": [
    "# Parameters for DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcCOViZyOUsd",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64                   \n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.1\n",
    "EPSILON_DECAY = 30000\n",
    "GAMMA = 0.4                    \n",
    "TARGET_REPLACE_ITR = 10 # T                      \n",
    "MEMORY_CAPACITY = 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t91-FwkrOUse"
   },
   "source": [
    "# DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_P96tHI6OUse",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_size):\n",
    "        self.state_size = state_size # number of previous days * number of features\n",
    "        self.step = 0\n",
    "        self.action_size = 3 # buy, sell, hold\n",
    "        self.memory = ReplayMemory(MEMORY_CAPACITY) \n",
    "        self.epsilon = EPSILON\n",
    "        self.epsilon_min = EPSILON_MIN\n",
    "        self.epsilon_decay = EPSILON_DECAY\n",
    "        self.policy_net = Network(state_size, self.action_size).to(device)\n",
    "        self.target_net = Network(state_size, self.action_size).to(device)\n",
    "        \n",
    "        self.target_net.eval() # fix the target net\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=1e-3)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=EPISODES)\n",
    "\n",
    "\n",
    "    def act(self, state, hold_num, is_train=True):\n",
    "        self.step += 1\n",
    "        \n",
    "        # calculate the epsilon threshold\n",
    "        # epsilon decays as more steps are completed\n",
    "        eps_threshold = self.epsilon_min + (self.epsilon - self.epsilon_min) * math.exp(-1. * self.step / self.epsilon_decay)\n",
    "        \n",
    "        # the agent will randomly select an action in the possiblility of epsilon\n",
    "        if is_train and np.random.rand() <= eps_threshold:\n",
    "            action_space = [0, 1, 2]\n",
    "            return torch.tensor([random.sample(action_space, 1)], device=device)\n",
    "        \n",
    "        # the agent will choose action based on the output of the policy net with 1-epsilon possibility\n",
    "        with torch.no_grad():\n",
    "            if is_train == False:\n",
    "                self.policy_net.eval()\n",
    "            options = self.policy_net(state)\n",
    "            if is_train:\n",
    "                self.policy_net.train()\n",
    "\n",
    "        best_option = options[0]\n",
    "    \n",
    "        return options.max(1)[1].view(1, 1)\n",
    "\n",
    "\n",
    "    def optimize(self, train=True):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        if train:\n",
    "            self.policy_net.train()\n",
    "        else:\n",
    "            self.policy_net.eval()\n",
    "\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "\n",
    "        # calculate a mask for non-final states & concatenate batches\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        \n",
    "        # Compute all next states\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "        # Compute the expected Q values based on the target net\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "        \n",
    "        # Compute Huber loss\n",
    "        loss = 0\n",
    "        if train:\n",
    "            # criterion = nn.MSELoss()\n",
    "            # loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "            loss = F.smooth_l1_loss(state_action_values.float(), expected_state_action_values.unsqueeze(1).float())\n",
    "            \n",
    "            # Optimize the model\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scheduler.step()\n",
    "        return loss, float(self.optimizer.param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfluPrTpOUsi"
   },
   "source": [
    "# Parameters for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjRn4FIGOUsj"
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVgBu7M6OUsj"
   },
   "outputs": [],
   "source": [
    "# Simulation of the real stock environment\n",
    "class stock:\n",
    "\n",
    "    def __init__(self, scaled_df, unscaled_df, num_columns, init_money=100000, lookback_period=6, t=0):\n",
    "\n",
    "        self.n_actions = 3 # [buy, sell, hold]\n",
    "        self.n_features = lookback_period * num_columns # num of previous days * num of features types\n",
    "        self.lookback_period = lookback_period \n",
    "        self.init_money = init_money # initial money\n",
    "        \n",
    "        self.trend = scaled_df.values # normalized stock data\n",
    "        self.unscaled_trend = unscaled_df.values # prices w/o min-max normalization\n",
    "        \n",
    "        self.min_buy_commission = 5  # to allow MIN(min buy commission, vol * buy commission rate)\n",
    "        self.min_sell_commission = 5\n",
    "        self.buy_commission_rate = 0.0003\n",
    "        self.sell_commission_rate = 0.0003 \n",
    "        self.tax = 0.001 # applied when selling stocks\n",
    "        \n",
    "        self.t = t\n",
    "    \n",
    "    # reset the environment for each episode\n",
    "    def reset(self):\n",
    "        self.hold_money = self.init_money # reset holding money to initial money\n",
    "        \n",
    "        self.buy_price = 0                # previous stock buying price\n",
    "        self.sell_price = 0               # previous stock selling price\n",
    "        \n",
    "        self.stock_value = 0              # total market value of stock\n",
    "        self.total_value = 0              # total stock value + cash\n",
    "        self.last_value = self.init_money # total value in t-1\n",
    "        self.hold_num = 0                 # num of stocks currently hold\n",
    "        self.hold_period = 0              # num of consecutive action 0 has been called\n",
    "        self.total_profit = 0             # total profits so far\n",
    "        self.reward = 0                   # reward at current timestamp\n",
    "        \n",
    "        self.consecutive_action = 0       # number of consecutive actions called \n",
    "        self.pre_action = -1              # action chosen in previous step\n",
    "        \n",
    "        # record the timing for each executed actions\n",
    "        self.time_buy_stock = [] \n",
    "        self.time_sell_stock = [] \n",
    "        self.time_hold_stock = []\n",
    "        \n",
    "        # performance recorder\n",
    "        self.profit_rate_account = [] \n",
    "        self.profit_rate_stock = []\n",
    "        self.daily_return_account = []\n",
    "        \n",
    "        self.t = 0\n",
    "\n",
    "        return self.get_state(self.t)\n",
    "\n",
    "    def get_state(self, t):\n",
    "\n",
    "        day = t - self.lookback_period + 1\n",
    "\n",
    "        historical = []\n",
    "\n",
    "        # if the current timestamp < lookback period, pad with the first day data\n",
    "        if t < self.lookback_period - 1:\n",
    "            historical = np.concatenate((-(t - self.lookback_period + 1) * [self.trend[0, :]], self.trend[0 : t + 1, :]))\n",
    "        else:\n",
    "            historical = self.trend[t - self.lookback_period + 1 : t + 1]\n",
    "        \n",
    "        # data at t+1\n",
    "        if t < self.lookback_period - 2:\n",
    "            historical2 = np.concatenate((-(t - self.lookback_period + 2) * [self.trend[0, :]], self.trend[0 : t + 2, :]))\n",
    "        else:\n",
    "            historical2 = self.trend[t - self.lookback_period + 2 : t + 2]\n",
    "\n",
    "        historical = (historical2 / (historical + 0.0001) - 1) \n",
    "        historical = historical.reshape(1, -1)\n",
    "\n",
    "        return torch.FloatTensor(historical).to(device) # return states @ time t\n",
    "\n",
    "    def buy_stock(self):       \n",
    "        \n",
    "        # buy stocks in batches of 10 -> if the remaining money cannot buy 10 stocks at once,\n",
    "        # then the buy action cannot be carried out\n",
    "        buy_num = (self.hold_money // self.unscaled_trend[self.t] // 10) * 10\n",
    "        self.buy_price = self.unscaled_trend[self.t]\n",
    "        \n",
    "        volume = self.unscaled_trend[self.t] * buy_num # current stock price * trading Q\n",
    "\n",
    "        commission = max(volume * self.buy_commission_rate, self.min_buy_commission)\n",
    "\n",
    "        # if the commission cannot be paid, then reduce buying num by 10\n",
    "        while commission + volume > self.hold_money and buy_num > 0:\n",
    "            buy_num -= 10\n",
    "            volume -= 10 * self.unscaled_trend[self.t]\n",
    "            commission = max(volume * self.buy_commission_rate, self.min_buy_commission)\n",
    "        \n",
    "        # if a buy action is executed, update the portfolio\n",
    "        if buy_num > 0:\n",
    "            self.hold_num += buy_num\n",
    "            self.stock_value += volume\n",
    "            \n",
    "            # update holding money by lessing stock costs and commission fees\n",
    "            self.hold_money = self.hold_money - volume - commission \n",
    "\n",
    "            self.time_buy_stock.append(self.t) # record the buy action\n",
    "\n",
    "    def sell_stock(self, sell_num):\n",
    "        volume = sell_num * self.unscaled_trend[self.t]\n",
    "\n",
    "        self.sell_price = self.unscaled_trend[self.t]\n",
    "\n",
    "        commission = max(volume * self.sell_commission_rate, self.min_buy_commission)\n",
    "        sell_tax = self.tax * volume\n",
    "        \n",
    "        # update holding money by adding incomes from stocks and lessing applicable commission and tax\n",
    "        self.hold_money = self.hold_money + volume - commission - sell_tax\n",
    "        self.hold_num = 0\n",
    "        self.stock_value = 0\n",
    "        self.time_sell_stock.append(self.t)\n",
    "\n",
    "    def step(self, action):\n",
    "        # check if the agent repeatedly calls the same action\n",
    "        if action == self.pre_action:\n",
    "            self.consecutive_action += 1\n",
    "        else:\n",
    "            self.consecutive_action = 0\n",
    "            self.pre_action = action\n",
    "        \n",
    "        # carries out the action\n",
    "        if action == 1:\n",
    "            self.buy_stock()\n",
    "            self.hold_period = 0\n",
    "\n",
    "        elif action == 2 and self.hold_num > 0:\n",
    "            self.hold_period = 0\n",
    "            self.sell_stock(self.hold_num)\n",
    "        else:\n",
    "            self.time_hold_stock.append(self.t)\n",
    "            self.hold_period += 1\n",
    "        \n",
    "        # current market value of holding stocks\n",
    "        self.stock_value = self.unscaled_trend[self.t] * self.hold_num\n",
    "        \n",
    "        self.total_value = self.stock_value + self.hold_money   # total portfolio value \n",
    "        self.total_profit = self.total_value - self.init_money  # profits above initial funds\n",
    "        \n",
    "        # basic reward\n",
    "        self.reward = (self.total_value / self.last_value - 1)\n",
    "        self.daily_return_account.append(self.reward)\n",
    "        \n",
    "        # scaled reward calculation to encourage \"buy-low-sell-high\"\n",
    "        if action == 1 and self.sell_price != 0 and self.hold_num == 0:\n",
    "            if self.reward > 0:\n",
    "                self.reward /= (self.sell_price / self.unscaled_trend[self.t])\n",
    "            else:\n",
    "                self.reward *= self.sell_price / self.unscaled_trend[self.t]\n",
    "        elif action == 2 and self.hold_num > 0:\n",
    "            if self.reward > 0:\n",
    "                self.reward *= self.unscaled_trend[self.t] / self.buy_price\n",
    "            else:\n",
    "                self.reward /= (self.unscaled_trend[self.t] / self.buy_price)\n",
    "        \n",
    "        # applies penalties on repeated actions\n",
    "        if self.consecutive_action > 0:\n",
    "            if self.pre_action == 1 or self.pre_action == 2:\n",
    "                if self.reward > 0:\n",
    "                    self.reward *= 0.25 ** self.consecutive_action\n",
    "                else:\n",
    "                    self.reward = self.reward * min(sys.float_info.max / 2, 4 ** self.consecutive_action)\n",
    "            self.reward -= self.hold_period ** 1.1\n",
    "        \n",
    "        self.last_value = self.total_value\n",
    "        \n",
    "        self.profit_rate_account.append(self.total_value / self.init_money - 1)\n",
    "        self.profit_rate_stock.append(self.unscaled_trend[self.t] / self.unscaled_trend[0] - 1)\n",
    "\n",
    "        self.t = self.t + 1\n",
    "        s_ = self.get_state(self.t)\n",
    "\n",
    "        return s_, self.reward, self.t == len(self.trend) - 2\n",
    "    \n",
    "    def get_info(self):\n",
    "        return self.time_sell_stock, self.time_buy_stock, self.time_hold_stock, self.profit_rate_account, self.profit_rate_stock, self.daily_return_account  \n",
    "    \n",
    "    def calc_sharpe(self):\n",
    "        # calculation of sharpe ratio\n",
    "        daily_return = np.array(self.daily_return_account)\n",
    "        return np.mean(daily_return) / np.std(daily_return) * (252 ** 0.5)\n",
    "    \n",
    "    def draw(self, save_name1, save_name2):\n",
    "\n",
    "        time_sell_stock, time_buy_stock, time_hold_stock, profit_rate_account, profit_rate_stock, daily_return = self.get_info()\n",
    "        \n",
    "        total_gains = self.total_profit   # total gains in dollars\n",
    "        roi = profit_rate_account[-1]     # final ROI\n",
    "        sharpe = self.calc_sharpe()       # sharpe ratio\n",
    "\n",
    "        stock_price = self.unscaled_trend\n",
    "        fig = plt.figure(figsize = (15,5))\n",
    "        plt.plot(stock_price, color='r', lw=2.)\n",
    "        \n",
    "        # plot buy, sell, (hold) actions by time, on stock's trend\n",
    "        # uncomment for plotting hold action\n",
    "        plt.plot(stock_price, '^', label = 'buy action', markevery = time_buy_stock, color='m', markersize=8)\n",
    "        plt.plot(stock_price, 'v', label = 'sell action', markevery = time_sell_stock, color='k', markersize=8)\n",
    "        # plt.plot(stock_price, '>', label = 'hold action', markevery = time_hold_stock, color='b', markersize=8)\n",
    "        \n",
    "        plt.title('total gains %.2f, ROI %.2f%%, sharpe ratio %.2f'%(total_gains, roi * 100, sharpe))\n",
    "        plt.legend()\n",
    "        display(fig)\n",
    "        plt.savefig(save_name1)\n",
    "        plt.close()\n",
    "\n",
    "        fig = plt.figure(figsize = (15,5))\n",
    "        # plot the stock performance comparisons\n",
    "        plt.plot(profit_rate_account, label='my account')\n",
    "        plt.plot(profit_rate_stock, label='stock')\n",
    "        plt.legend()\n",
    "        display(fig)\n",
    "        plt.savefig(save_name2)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_tjEFhUOUsm"
   },
   "source": [
    "# TRAIN Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zD8m0IgYOUso"
   },
   "outputs": [],
   "source": [
    "def game_step(observation, env, step=None, train=True):\n",
    "\n",
    "    # agent takes action based on observation \n",
    "    action = agent.act(observation, env.hold_num, train)\n",
    "\n",
    "    # after taking the action, observing the next observation and reward\n",
    "    observation_, reward, done = env.step(action)\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    \n",
    "    # push memory to replay memory\n",
    "    if train:\n",
    "        agent.memory.push(observation, action, reward, observation_)\n",
    "    loss = 0\n",
    "    lr = 0\n",
    "    \n",
    "    if step and (step >= BATCH_SIZE) and (step % 5 == 0):\n",
    "        loss, lr = agent.optimize(train)\n",
    "\n",
    "    # swap observation\n",
    "    observation = observation_\n",
    "    \n",
    "    return observation, done, loss, lr\n",
    "    \n",
    "\n",
    "def run(sharpe_matrix, epoch):\n",
    "    max_profit = -99999999\n",
    "    best_loss = 99999999\n",
    "    step = 0\n",
    "\n",
    "    for episode in range(EPISODES):\n",
    "        \n",
    "        # initial observation\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        total_loss = 0\n",
    "        \n",
    "        while not done:\n",
    "            observation, done, loss, lr = game_step(observation, env, step=step)\n",
    "            total_loss += loss\n",
    "            if not done:\n",
    "                step += 1\n",
    "                \n",
    "        if env.total_profit > max_profit:\n",
    "            torch.save(agent.policy_net.state_dict(), 'best_model')\n",
    "            max_profit = env.total_profit\n",
    "        \n",
    "        daily_ret = env.daily_return_account\n",
    "        daily_ret = np.array(daily_ret)\n",
    "        sharpe = np.mean(daily_ret) / np.std(daily_ret) * (252 ** 0.5)\n",
    "        \n",
    "        sharpe_matrix[epoch][episode] = sharpe # for plotting performance graphs over episodes\n",
    "        \n",
    "        print('episode:%d, total_profit:%.3f, sharpe:%.3f, loss:%.3f, lr:%.5f' % (episode, env.total_profit, sharpe, total_loss, lr))\n",
    "        \n",
    "        if episode % TARGET_REPLACE_ITR == 0:\n",
    "            agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "    \n",
    "    return sharpe_matrix\n",
    "\n",
    "def BackTest():\n",
    "    # validation\n",
    "    env_val = stock(combined_scaled_data.iloc[-end:].reset_index(drop=True), \n",
    "                    combined_raw_data['Adj Close'].iloc[-end:].reset_index(drop=True), num_col, lookback_period = LOOKBACK_PERIODS)\n",
    "    observation = env_val.reset()\n",
    "\n",
    "    agent.policy_net.load_state_dict(torch.load('best_model'))\n",
    "    while True:\n",
    "        observation, done, _, _ = game_step(observation, env_val, train=False)\n",
    "        if done:\n",
    "            break\n",
    "    print('total_profit:%.3f' % (env_val.total_profit))\n",
    "    return env_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEi8CJDOOUsp"
   },
   "source": [
    "# Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GXGEizKEOUsq",
    "outputId": "79cee825-93fa-43ab-bee3-ae271429a39f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LOOKBACK_PERIODS = 5\n",
    "EPISODES = 5\n",
    "EPOCHS = 5\n",
    "\n",
    "start = int(len(combined_scaled_data) * 0.4)\n",
    "end = int(len(combined_scaled_data) * 0.1)\n",
    "\n",
    "roi = []\n",
    "sharpe = []\n",
    "\n",
    "sharpe_matrix = np.zeros([EPOCHS, EPISODES])\n",
    "\n",
    "num_col = len(combined_scaled_data.columns)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    env = stock(combined_scaled_data.iloc[:-end], combined_raw_data['Adj Close'].iloc[:-end], num_col, lookback_period = LOOKBACK_PERIODS)\n",
    "    agent = Agent(env.n_features)\n",
    "    sharpe_matrix = run(sharpe_matrix, epoch)\n",
    "    #env.draw('train_trade.png', 'train_profit.png') # optional: plot for training result\n",
    "    agent.step\n",
    "    \n",
    "    # validation\n",
    "    env_val = BackTest()\n",
    "    roi.append(env_val.profit_rate_account[-1]*100)\n",
    "    sharpe.append(env_val.calc_sharpe())\n",
    "    env_val.draw('trade1.png', 'profit1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average ROI: ', (np.sort(roi)).mean())\n",
    "print('Average Sharpe Ratio: ', np.sort(sharpe).mean())\n",
    "print('ROI Stdev: ', np.std(roi))\n",
    "print('Sharpe Ratio Stdev: ', np.std(sharpe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all ROI & Sharpe Ratio\n",
    "np.sort(roi), np.sort(sharpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sharpe ratio performance (with error bar) for episodes\n",
    "# sharpe_matrix: (EPOCH, EPISODE)\n",
    "sharpe_table = pd.DataFrame(columns=['EPOCHS', 'EPISODES', 'SHARPE'])\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for episo in range(EPISODES):\n",
    "        row = pd.DataFrame(np.array([epoch, episo, sharpe_matrix[epoch][episo]]).reshape(1, 3), columns=['EPOCHS', 'EPISODES', 'SHARPE'])\n",
    "        sharpe_table = sharpe_table.append(pd.DataFrame(row, columns=['EPOCHS', 'EPISODES', 'SHARPE']))\n",
    "sharpe_table['EPOCHS'] = sharpe_table['EPOCHS'].astype('int64', copy=True)\n",
    "sharpe_table['EPISODES'] = sharpe_table['EPISODES'].astype('int64', copy=True)\n",
    "sharpe_table = sharpe_table.reset_index(drop=True)\n",
    "sns.set(rc={'figure.figsize':(10, 10)})\n",
    "sharpe_plot = sns.lineplot(data=sharpe_table, x='EPISODES', y='SHARPE')\n",
    "sharpe_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGs6Asr1OUsr"
   },
   "source": [
    "# Comparison Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate performance for buy-and-hold strategy\n",
    "bh = combined_raw_data['Adj Close'].iloc[-end:].reset_index(drop=True).values\n",
    "bh_roi = bh[-1] / bh[0] - 1\n",
    "bh_daily_ret = []\n",
    "for i in range(len(bh) - 1):\n",
    "    bh_daily_ret.append(bh[i+1] / bh[i] - 1)\n",
    "bh_sharpe = np.mean(bh_daily_ret) / np.std(bh_daily_ret) * (252 ** 0.5)\n",
    "\n",
    "print('Buy-and-Hold ROI:', bh_roi * 100)\n",
    "print('Buy-and-Hold Sharpe Ratio:', bh_sharpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate performance for short-and-hold strategy\n",
    "sh = bh\n",
    "sh_roi = sh[0] / sh[-1] - 1\n",
    "sh_daily_ret = []\n",
    "for i in range(len(sh) - 1):\n",
    "    sh_daily_ret.append(sh[i] / sh[i+1] - 1)\n",
    "sh_sharpe = np.mean(sh_daily_ret) / np.std(sh_daily_ret) * (252 ** 0.5)\n",
    "\n",
    "print('Short-and-Hold ROI:', sh_roi * 100)\n",
    "print('Short-and-Hold Sharpe Ratio:', sh_sharpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Separate Validation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7A_NuJAOUsr"
   },
   "outputs": [],
   "source": [
    "# allows for running validation individually\n",
    "env_val = BackTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "id": "99-IgSUvOUss",
    "outputId": "3592ec24-791b-4d3e-b017-0db93d9e0cc5"
   },
   "outputs": [],
   "source": [
    "# draw\n",
    "env_val.draw('trade1.png', 'profit1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBq0P2f65Kad",
    "outputId": "10117ea6-29e4-4ce5-8f50-402ec8542cfc"
   },
   "outputs": [],
   "source": [
    "temp = np.array([[0.9, 0.8, 0.3]])\n",
    "np.argsort(temp, 1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQgijwlk5Kad"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DQN (4).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
